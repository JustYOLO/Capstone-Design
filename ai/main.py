import httpx
import chromadb
import ollama
import ast
import os
from flask import Flask, request, Response, jsonify
import json
import redis
import re

app = Flask(__name__)

rd = redis.StrictRedis(host='chat_log', port=6666, db=0)

client = ollama.Client(host='http://host.docker.internal:11434')

client_ko = chromadb.PersistentClient(path="/app/chromadb_storage_ko")
collection_ko = client_ko.get_or_create_collection(name="flowers_ko")


def extract_keywords(situation: str) -> list:
    prompt = f"""
    ë‹¤ìŒ ë¬¸ì¥ì—ì„œ í•µì‹¬ ì˜ë¯¸ë¥¼ ë‹´ì€ ë‹¨ì–´(ì˜ˆ: ê°ì •, ëª©ì , ê´€ê³„ ë“±)ë¥¼ 3ê°œ ì¶”ì¶œí•´ì¤˜. ê¼­ ë¬¸ì¥ ë‚´ ë‹¨ì–´ê°€ ì¡´ì¬í•˜ì§€ ì•Šì•„ë„ ê´œì°®ë‹¤. 
    ë„ˆê°€ ìƒí™©ì„ ìœ ì¶”í•˜ì—¬ ê½ƒ ì¶”ì²œì— ì¤‘ì‹¬ì´ ë˜ëŠ” ë‹¨ì–´ë§Œ ë½‘ì•„ì¤˜. 
    ì ˆëŒ€ 'ì„ ë¬¼', 'ê½ƒ' ë‹¨ì–´ëŠ” ë½‘ì§€ë§ˆ.

    ë¬¸ì¥: "{situation}"

    ## ì¶œë ¥ ì˜ˆì‹œ
    - í‚¤ì›Œë“œ: ["ì¡´ê²½", "êµìˆ˜ë‹˜", "ê°ì‚¬"]
    - í‚¤ì›Œë“œ: ["ê°ì‚¬", "ì–´ë²„ì´", "ë¶€ëª¨ë‹˜"]
    - í‚¤ì›Œë“œ: ["ì‚¬ë‘", "ê¸°ë…ì¼", "ì—°ì¸"]
    
    ì ˆëŒ€ 'ì„ ë¬¼', 'ê½ƒ' ë‹¨ì–´ëŠ” ë½‘ì§€ë§ˆ.

    ë°˜ë“œì‹œ Python ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´.
    """
    output = client.generate(model="gemma3:4b", prompt=prompt)["response"]
    print(f"[í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼]: {output.strip()}")

    try:
        # ë¦¬ìŠ¤íŠ¸ ì•ˆì˜ ë¬¸ìì—´ë§Œ ì¶”ì¶œ
        matches = re.findall(r'"(.*?)"', output)
        return matches
    except Exception as e:
        print(f"[keyword parse error] {e}")
        return []



def search_flower_ko(situation: str) -> list:
    keywords = extract_keywords(situation)
    print(f"[ì¶”ì¶œëœ í‚¤ì›Œë“œ]: {keywords}")

    all_candidates = []
    for keyword in keywords:
        print(f"[ğŸ” ê²€ìƒ‰ ê¸°ì¤€ í‚¤ì›Œë“œ]: {keyword}")
        query_embedding = client.embeddings(model="llama3-ko:latest", prompt=keyword)["embedding"]
        results = collection_ko.query(
            query_embeddings=[query_embedding],
            n_results=10  
        )
        docs = results["documents"][0]
        print(f"[{keyword} í›„ë³´]: {docs}")
        all_candidates.extend(docs)

    # ì¤‘ë³µ ì œê±°
    all_candidates = list(dict.fromkeys(all_candidates))

    flowers_info = "\n".join(all_candidates)
    
    # ìµœì¢… ì¶”ì²œ 3ê°œë¥¼ LLMìœ¼ë¡œ ì¬ì •ë ¬
    prompt = f"""
    ìƒí™©: "{situation}"
    
    ë‹¹ì‹ ì€ ê½ƒ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ìš”ì²­í•œ ìƒí™©ì— ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ëŠ” ê½ƒì„ ê°ì •ì ìœ¼ë¡œ ì˜ ì¶”ì²œí•´ì£¼ì„¸ìš”.
    ì•„ë˜ëŠ” ì¶”ì²œ í›„ë³´ ê½ƒê³¼ ê·¸ ê½ƒë§ì…ë‹ˆë‹¤.

    {flowers_info}
    
    ì´ ì¤‘ì—ì„œ ìƒí™©ì— ê°€ì¥ ì˜ ì–´ìš¸ë¦¬ë©´ì„œ ê½ƒì§‘ì—ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” ê½ƒ 3ê°œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.

    ## ì¶œë ¥ í˜•ì‹: (ì´ìœ ëŠ” ì ˆëŒ€ ì¶”ì²œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)
    - ê½ƒ: ê½ƒë§
    
    ## ì˜ˆì‹œ ì¶œë ¥:
    - ë¶‰ì€ ë™ë°±ê½ƒ: ë‚˜ëŠ” ë‹¹ì‹ ì´ ëˆ„êµ¬ë³´ë‹¤ë„ ì•„ë¦„ë‹µë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.
    - í° ì¥ë¯¸: ë‹¹ì‹ ì˜ ìˆœìˆ˜í•¨ê³¼ ì§„ì‹¤í•¨ì„ ì¡´ê²½í•©ë‹ˆë‹¤.
    - ë…¸ë€ í•´ë°”ë¼ê¸°: ë‹¹ì‹ ì˜ ë°ì€ ì—ë„ˆì§€ê°€ ì£¼ë³€ì„ í™˜í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
    """
    
    response = client.generate(model="gemma3:4b", prompt=prompt)["response"]

    print(f"[ì¶”ì²œëœ ê½ƒ ëª©ë¡]: {response.strip()}")

    final_result = []
    for line in response.strip().split("\n"):
        if ":" in line:
            name, reason = line.split(":", 1)
            final_result.append((name.strip(), reason.strip()))
    return final_result



def generate_flower_recommendation_ko(situation, recommended_flowers):
    """ì¶”ì²œëœ ê½ƒ ëª©ë¡ì„ ë°”íƒ•ìœ¼ë¡œ Ollamaë¥¼ ì´ìš©í•´ ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ì²œ ë¬¸ì¥ ìƒì„±"""
    if not recommended_flowers:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì ˆí•œ ê½ƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    flowers_info = ", ".join([f"{f}({m})" for f, m in recommended_flowers])
    
    prompt_text = f"""
        You are a flower expert. The user has requested a flower recommendation for the following situation:
        Situation: {situation}

        Here are the recommended flowers and their meanings:
        {flowers_info}

        Please write a kind, heartfelt, and logically clear recommendation that explains how the meanings of these flowers relate to the user's situation.

        ## Output Format
        
        - ì¢…í•© ì¶”ì²œ ì´ìœ :
        [Explain logically and kindly how the combined meanings of the recommended flowers connect specifically to the user's situation.]

        Important Instructions:
        - Provide ONLY the 'ì¢…í•© ì¶”ì²œ ì´ìœ ' section.
        - Do NOT include any other sections or titles.
        - Respond strictly in Korean.
        - And there must be a new line between each flower descriptions.
    """
    
    # ì‘ë‹µ ìƒì„±
    output = client.generate(model="gemma3:4b", prompt=prompt_text)
    return output["response"]






def format_recommended_flowers(recommended_flowers_ko):
    """ì£¼ì–´ì§„ recommended_flowers_ko ë°ì´í„°ë¥¼ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    formatted_lines = ["ì¶”ì²œí•˜ëŠ” ê½ƒê³¼ ê½ƒë§:"]
    for flower, reason in recommended_flowers_ko:
        formatted_lines.append(f"{flower}: {reason}")
    return "\n\n".join(formatted_lines)




@app.route("/api/generate", methods=["POST"])
def recommend():
    data = request.get_json()
    situation = data.get("prompt")

    if not situation:
        return Response(
            json.dumps({"error": "situation is required"}, ensure_ascii=False),
            content_type='application/json'
        ), 400

    print("DEBUG:", situation)
    
    flowers = search_flower_ko(situation)
    
    print("DEBUG:", flowers)

    result = generate_flower_recommendation_ko(situation, flowers)
    print("DEBUG:", result)
    flowers = format_recommended_flowers(flowers)
    flowers = flowers + "\n" + "\n" + result 
    print("DEBUG:", flowers)
    return Response(
        flowers,
        content_type='text/plain'
    ), 200


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=8111)